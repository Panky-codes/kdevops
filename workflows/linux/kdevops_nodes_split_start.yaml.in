---
vagrant_global:
  box: "@VAGRANTBOX@"
  box_version: "@VBOXVERSION@"
  skip_ansible: @SKIPANSIBLE@
  memory: 8192
  cpus: 8
  enable_zns: @VAGRANTENABLEZNS@
  # For creating extra nvme drives and if libvirt is used it is also
  # where we let vagrant store its images.
  storage_pool_path: '@KDEVOPSSTORAGEPOOLPATH@'
  # Enable the below options
  #limit_boxes: "yes"
  #limit_num_boxes: 1
  # You can use force_provider if your OS defaults differ from the default
  # heuristics on Vagrantfile.
  #force_provider: "virtualbox"
  virtualbox_cfg:
    auto_update: false
    enabled: "true"
    enable_sse4: "true"
    # can be vdi, vmdk, vhd
    nvme_disk_postfix: 'vdi'
    # To stress test a virtual nvme controller you could peg all disks onto
    # one controller. We want to avoid this as our focus is testing filesystems
    # and not storage controllers however Virtualbox currently only supports
    # one nvme storage controller. Set this to true only if you are adding
    # support for this upstream to Virtualbox.
    nvme_controller_per_disk: false
  libvirt_cfg:
    nvme_disk_postfix: 'qcow2'
    nvme_disk_id_prefix: 'drv'
    # This lets the ansible role kdevops_vagrant try to infer your default
    # distro group to use for qemu. OpenSUSE and Fedora uses qemu here by
    # default, debian uses libvirt-qemu. You can always override with the
    # environment variable KDEVOPS_VAGRANT_QEMU_GROUP. If not sure edit
    # /etc/libvirt/qemu.conf user and group settings. If using apparmor /
    # selinux you may run into snags, but that is out of scope of this project.
    qemu_group_auto: true
    qemu_group: 'libvirt-qemu'
    emulator_path: '@QEMUBINPATH@'
    storage_pool_path: '@LIBVIRTSTORAGEPOOLPATH@'
    uri: '@LIBVIRTURI@'
    system_uri: '@LIBVIRTSYSTEMURI@'
    session: @LIBVIRTSESSION@
    session_socket: '@LIBVIRTSESSIONSOCKET@'
    session_management_network_device: '@LIBVIRTSESSIONMANAGEMENTNETWORKDEVICE@'
    session_public_network_dev: '@LIBVIRTSESSIONPUBLICNETWORKDEV@'
  # On the current directory
  nvme_disk_path: '.vagrant/nvme_disks/'
  nvme_disk_prefix: 'nvme_disk'
  # This ends up slightly different depending on the vagrant provider right now.
  # For Virtualbox: /dev/nvme0n1, /dev/nvme0n2, etc.
  # For libvirt:    /dev/nvme0n1, /dev/nvme1n1, etc.
  # This is due to how Virtualbox only supports one nvme storage controller
  nvme_disks:
    data:
      size: 102400
    scratch:
      size: 102400
    extra1:
      size: 102400
    extra2:
      size: 102400
    zone1:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone2:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone3:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone4:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone5:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone6:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone7:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone8:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone9:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone10:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone11:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone12:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone13:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@
    zone14:
      size: @NVMEZONEDRIVESIZE@
      zoned: true
      zone_zasl: @NVMEZONEZASL@
      zone_size: @NVMEZONESIZE@
      zone_capacity: @NVMEZONECAPACITY@
      zone_max_active: @NVMEZONEMAXACTIVE@
      zone_max_open: @NVMEZONEMAXOPEN@
      physical_block_size: @NVMEZONEPHYSICALBLOCKSIZE@
      logical_block_size: @NVMEZONELOGICALBLOCKSIZE@

# Note: vagrant is not a fan of hosts with underscores.
#
# Modify the hostname to include a purpose, and then extract it later with
# ansible, for instance with:
#
# section: "{{ ansible_hostname | regex_replace('kdevops-') | regex_replace('-dev') | regex_replace('-', '_') }}"
#
# So if you say embraced kdevops-pokeyman and kdevops-pokeyman-dev you'd end up
# getting in the section always as pokeyman. As is below, with the above
# ansible regex you'd get the digits.
vagrant_boxes:
