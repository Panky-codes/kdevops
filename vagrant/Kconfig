if VAGRANT

choice
	prompt "Vagrant virtualization technology to use"
	default VAGRANT_LIBVIRT

config VAGRANT_LIBVIRT
	bool "Libvirt"
	help
	  Select this option if you want to use KVM / libvirt for
	  local virtualization.

config VAGRANT_VIRTUALBOX
	bool "Virtualbox"
	help
	  Select this option if you want to use Virtualbox for
	  local virtualization.

endchoice

if VAGRANT_LIBVIRT

config QEMU_USE_DEVELOPMENT_VERSION
	bool "Should we look for a development version of qemu?"
	help
	  You want to enable this option if for example the currently
	  available version of qemu does not yet have support for the feature
	  you are going to be working on.

	  Say yes here if you are compiling your own version of qemu.

config QEMU_BIN_PATH_LIBVIRT
	string "Qemu binary path to use"
	default "/usr/local/bin/qemu-system-x86_64" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin/qemu-system-x86_64" if !QEMU_USE_DEVELOPMENT_VERSION

choice
	prompt "Libvirt storage pool path"
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO if !DISTRO_SUSE
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the libvirt storage pool path where
	  we'll download images and store images for guests spawned. If users
	  git cloned kdevops somewhere in their home directory they'll have to
	  make sure that the group which libvirt is configured to run for their
	  distribution can have access to that directory. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	bool "Use the same path typically used by each distribution"
	help
	  Select this option if you want to use the same location as the
	  distribution would typically use. We expect this to be
	  /var/lib/libvirt/images/ for most distributions, however we can
	  customize this further if this is not true by adding further checks.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the qemu storage pool path. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM
	string "Libvirt storage pool path"
	default "/var/lib/libvirt/images/" if !LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default $(shell, scripts/cwd-append.sh vagrant) if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default "/opt/libvirt/images/" if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	help
	  The path to use for the libvirt storage pool path. Since kdevops uses
	  vagrant for virtualization this is also the path used to place the
	  additional nvme drives created. kdevops adds a postfix "kdevops" to
	  this directory as it wants to allow vagrant full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will let vagrant store images in /var/lib/libvirt/images/ and
	  the nvme qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/.vagrant/nvme_disks/guest-hostname/.

endif

if VAGRANT_VIRTUALBOX

choice
	prompt "Virtualbox storage pool path"
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL if !DISTRO_SUSE
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the vagrant storage pool path where
	  additional nvme drives will be created. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the directory where we'll have kdevops create additional nvme drives
	  for virtualbox to use. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM
	string "Virtualbox storage pool path"
	default $(shell, scripts/cwd-append.sh vagrant) if VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD
	default "/opt/virtualbox/storage/" if VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL
	help
	  The path to use for creating additional nvme drives used by
	  virtualbox. kdevops adds a postfix "kdevops" to this directory as it
	  wants to allow vagrant full control over that directory. For instance
	  if this is /opt/virtualbox/storage/ kdevops will have virtualbox
	  create the nvme files under the directory
	  /opt/virtualbox/storage/kdevops/.vagrant/nvme_disks/guest-hostname/.

endif

config LIBVIRT_QEMU_GROUP
	string
	default "qemu" if !DISTRO_DEBIAN && !DISTRO_UBUNTU
	default "libvirt-qemu" if DISTRO_DEBIAN || DISTRO_UBUNTU

config KDEVOPS_STORAGE_POOL_PATH
	string
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM if VAGRANT_LIBVIRT
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM if VAGRANT_VIRTUALBOX

config QEMU_BIN_PATH
	string
	default QEMU_BIN_PATH_LIBVIRT if VAGRANT_LIBVIRT
	default "/usr/bin/qemu-system-x86_64" if !VAGRANT_LIBVIRT

config HAVE_SUSE_VAGRANT
	bool
	default $(shell, scripts/check_distro_kconfig.sh suse)

choice
	prompt "Vagrant Guest Linux distribution to use"
	default VAGRANT_DEBIAN if !HAVE_SUSE_VAGRANT
	default VAGRANT_SUSE if HAVE_SUSE_VAGRANT

config VAGRANT_DEBIAN
	bool "Debian"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG if FSTESTS_XFS
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL if FSTESTS_XFS
	help
	  This option will set the target guest to Debian.

config VAGRANT_OPENSUSE
	bool "OpenSUSE"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BLKTESTS_PREFERS_MANUAL if KDEVOPS_WORKFLOW_ENABLE_BLKTESTS
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	help
	  This option will set the target guest to OpenSUSE.

config VAGRANT_SUSE
	bool "SUSE"
	depends on HAVE_SUSE_VAGRANT
	select HAVE_KDEVOPS_CUSTOM_DEFAULTS
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_BLKTESTS_PREFERS_MANUAL if KDEVOPS_WORKFLOW_ENABLE_BLKTESTS
	select HAVE_DISTRO_SUSE
	select HAVE_DISTRO_PREFERS_REGISTRATION
	select HAVE_DISTRO_REG_METHOD_TWOLINE
	select VAGRANT_INSTALL_PRIVATE_BOXES
	select HAVE_CUSTOM_KDEVOPS_GIT
	select HAVE_CUSTOM_KDEVOPS_GIT_DATA
	select HAVE_CUSTOM_KDEVOPS_DIR
	help
	  This option will set the target guest to SUSE. There is currently
	  no scriptable way to download vagrant images, however the images
	  are available for download via:

	    https://suse.com/download

config VAGRANT_FEDORA
	bool "Fedora"
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	help
	  This option will set the target guest to Fedora.

endchoice

config HAVE_VAGRANT_BOX_VERSION
	bool
	default n

source "vagrant/Kconfig.debian"
source "vagrant/Kconfig.opensuse"
source "vagrant/Kconfig.fedora"

if HAVE_SUSE_VAGRANT
source "vagrant/Kconfig.suse"
endif # HAVE_SUSE_VAGRANT

config VAGRANT_BOX
	string "Vagrant box"
	default "debian/testing64" if VAGRANT_DEBIAN_TESTING64 || !VAGRANT
	default "debian/buster64" if VAGRANT_DEBIAN_BUSTER64
	default "opensuse/Tumbleweed.x86_64" if VAGRANT_OPENSUSE_X86_64_TW
	default "fedora/35-cloud-base" if VAGRANT_FEDORA_X86_64_35_CLOUD_BASE
	default "opensuse/Leap-15.3.x86_64" if VAGRANT_OPENSUSE_X86_64_LEAP_15_3
	default "Leap-15.4.x86_64" if VAGRANT_OPENSUSE_X86_64_LEAP_15_4
	default VAGRANT_SUSE_BOX if VAGRANT_SUSE
	help
	  The vagrant box to use.

config VAGRANT_PREFERRED_KERNEL_CI_SUBJECT_TOPIC
	string
	default VAGRANT_BOX if VAGRANT_DEBIAN_BUSTER64


config HAVE_VAGRANT_BOX_URL
	bool

if HAVE_VAGRANT_BOX_URL

config VAGRANT_BOX_URL
	string
	depends on HAVE_VAGRANT_BOX_URL
	default VAGRANT_SUSE_BOX_URL if HAVE_SUSE_VAGRANT
	default "https://download.opensuse.org/repositories/Virtualization:/Appliances:/Images:/openSUSE-Leap-15.4/images/boxes/Leap-15.4.x86_64.json" if VAGRANT_OPENSUSE_X86_64_LEAP_15_4

endif # HAVE_VAGRANT_BOX_URL

if HAVE_VAGRANT_BOX_VERSION

config VAGRANT_BOX_VERSION
	string "Vagrant box version"
	default "1.0.20210915" if VAGRANT_OPENSUSE_X86_64_TW_1020210915
	default "1.0.20210203" if VAGRANT_OPENSUSE_X86_64_TW_1020210203
	default "1.0.20200714" if VAGRANT_OPENSUSE_X86_64_TW_1020200714
	default "1.0.20210203" if VAGRANT_OPENSUSE_X86_64_TW_1020210203
	default "32.20200312.0" if VAGRANT_FEDORA_32_202003120
	help
	  The vagrant box version to use. This is set for you depending on the
	  image you select. You can manually override the version we have last
	  tested here.

endif # HAVE_VAGRANT_BOX_VERSION

if !HAVE_VAGRANT_BOX_VERSION

config VAGRANT_BOX_VERSION
	string
	default ""

endif # !HAVE_VAGRANT_BOX_VERSION

config VAGRANT_LIBVIRT_INSTALL
	bool "Installs libvirt"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the ansible role which installs
	  libvirt for you will be run. The goal will be to ensure you have
	  libvirt installed and running.

config VAGRANT_LIBVIRT_CONFIGURE
	bool "Configure libvirt so you spawn guests as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the ansible role which configures
	  libvirt for you will be run. This typically just requires adding the
	  user to a specific set of groups. The user must log out and back
	  in again, to ensure the new group takes effect. The goal in the
	  configuration will be to ensure you can use libvirt to spawn guests
	  as a regular user. You are encouraged to say y here unless you know
	  what you are doing or you already know this works. If you are unsure,
	  the litmus test for this is if you can run vagrant up, on any public
	  demo box available.

config VAGRANT_LIBVIRT_VERIFY
	bool "Verify that a user can spawn libvirt as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  To enable a user to be able to spawn libvirt guests as a regular user
	  a user is typically added to a few groups. These groups are not
	  effective immediately, and so before a user can assume that they
	  use vagrant they must verify that the required groups are effective.
	  If you enable this option, we will spawn an ansible role that will
	  verfify and ensure that your user is already part of these groups.
	  You can safely say yes here.

config VAGRANT_INSTALL_PRIVATE_BOXES
	bool "Install private vagrant boxes"
	default y
	help
	  If this option is enabled then the ansible role which installs
	  additional vagrant boxes will be run. This is useful if for example,
	  you have private vagrant boxes available and you want to use them.
	  You can safely disable this option if you are using only public
	  vagrant boxes. Enabling this option is safe as well, given no
	  private boxes would be defined, and so nothing is done.

config QEMU_ENABLE_NVME_ZNS
	bool "Enable Qemu NVMe ZNS drives"
	depends on VAGRANT_LIBVIRT
	default n
	help
	  If this option is enabled then you can enable NVMe ZNS drives on the
	  guests.

config QEMU_CUSTOM_NVME_ZNS
	bool "Customize Qemu NVMe ZNS settings"
	depends on QEMU_ENABLE_NVME_ZNS
	default n
	help
	  If this option is enabled then you will be able to modify the defaults
	  used for the 2 NVMe ZNS drives we create for you. By default we create
	  two NVMe ZNS drives with 100 GiB of total size, each zone being
	  128 MiB, and so you end up with 800 total zones. The zone capacity
	  equals the zone size. The default zone size append limit is also
	  set to 0, which means the zone append size limit will equal to the
	  maximum data transfer size (MDTS). The default logical and physical
	  block size of 4096 bytes is also used. If you want to customize any
	  of these ZNS settings for the drives we bring up enable this option.

	  If unsure say N.

if QEMU_CUSTOM_NVME_ZNS

config QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE
	int "Qemu ZNS storage nvme drive size"
	default 102400
	help
	  The size of the qemu nvme ZNS drive to expose. We expose 2 NVMe
	  ZNS drives of 100 GiB by default. This value chagnes its size.
	  100 GiB is a sensible default given most full fstests require about
	  50 GiB of data writes.

config QEMU_CUSTOM_NVME_ZONE_ZASL
	int "Qemu ZNS zasl - zone append size limit power of 2"
	default 0
	help
	  This is the zone append size limit. If left at 0 qemu will use
	  the maximum data transfer size (MDTS) for the zone size append limit.
	  Otherwise if this value is set to something other than 0, then the
	  zone size append limit will equal to 2 to the power of the value set
	  here multiplied by the minimum memory page size (4096 bytes) but the
	  qemu promises this value cannot exceed the maximum data transfer size.

config QEMU_CUSTOM_NVME_ZONE_SIZE
	string "Qemu ZNS storage nvme zone size"
	default "128M"
	help
	  The size the the qemu nvme ZNS zone size. The number of zones are
	  implied by the driver size / zone size. If there is a remainder
	  technically that should go into another zone with a smaller zone
	  capacity.

config QEMU_CUSTOM_NVME_ZONE_CAPACITY
	int "Qemu ZNS storage nvme zone capacity"
	default 0
	help
	  The size to use for the zone capacity. This may be smaller or equal
	  to the zone size. If set to 0 then this will ensure the zone
	  capacity is equal to the zone size.

config QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE
	int "Qemu ZNS storage nvme zone max active"
	default 0
	help
	  The max numbe of active zones. The default of 0 means all zones
	  can be active at all times.

config QEMU_CUSTOM_NVME_ZONE_MAX_OPEN
	int "Qemu ZNS storage nvme zone max open"
	default 0
	help
	  The max numbe of open zones. The default of 0 means all zones
	  can be opened at all times. If the number of active zones is
	  specified this value must be less than or equal to that value.

config QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCKSIZE
	int "Qemu ZNS storage nvme physical block size"
	default 4096
	help
	  The physical block size to use for ZNS drives. This ends up
	  what is put into the /sys/block/<disk>/queue/physical_block_size
	  and is the smallest unit a physical storage device can write
	  atomically. It is usually the same as the logical block size but may
	  be bigger. One example is SATA drives with 4KB sectors that expose a
	  512-byte logical block size to the operating system. For stacked
	  block devices the physical_block_size variable contains the maximum
	  physical_block_size of the component devices.

config QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCKSIZE
	int "Qemu ZNS storage nvme logical block size"
	default 4096
	help
	  The logical block size to use for ZNS drives. This ends up what is
	  put into the /sys/block/<disk>/queue/logical_block_size and the
	  smallest unit the storage device can address. It is typically 512
	  bytes.

endif # QEMU_CUSTOM_NVME_ZNS

config VAGRANT_ENABLE_ZNS
	bool
	default y if QEMU_ENABLE_NVME_ZNS

config QEMU_NVME_ZONE_DRIVE_SIZE
	int
	default 102400 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_ZASL
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_ZASL if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_SIZE
	string
	default "128M" if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_CAPACITY
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_CAPACITY if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_ACTIVE
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_OPEN
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_OPEN if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_PHYSICAL_BLOCKSIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCKSIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_LOGICAL_BLOCKSIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCKSIZE if QEMU_CUSTOM_NVME_ZNS

endif # VAGRANT
