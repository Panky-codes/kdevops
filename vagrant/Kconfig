if VAGRANT

choice
	prompt "Vagrant virtualization technology to use"
	default VAGRANT_LIBVIRT

config VAGRANT_LIBVIRT
	bool "Libvirt"
	help
	  Select this option if you want to use KVM / libvirt for
	  local virtualization.

config VAGRANT_VIRTUALBOX
	bool "Virtualbox"
	help
	  Select this option if you want to use Virtualbox for
	  local virtualization.

endchoice

if VAGRANT_LIBVIRT

config QEMU_USE_DEVELOPMENT_VERSION
	bool "Should we look for a development version of qemu?"
	help
	  You want to enable this option if for example the currently
	  available version of qemu does not yet have support for the feature
	  you are going to be working on.

	  Say yes here if you are compiling your own version of qemu.

config QEMU_BIN_PATH_LIBVIRT
	string "Qemu binary path to use"
	default "/usr/local/bin/qemu-system-x86_64" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin/qemu-system-x86_64" if !QEMU_USE_DEVELOPMENT_VERSION

choice
	prompt "Libvirt storage pool path"
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO if !DISTRO_SUSE
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the libvirt storage pool path where
	  we'll download images and store images for guests spawned. If users
	  git cloned kdevops somewhere in their home directory they'll have to
	  make sure that the group which libvirt is configured to run for their
	  distribution can have access to that directory. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	bool "Use the same path typically used by each distribution"
	help
	  Select this option if you want to use the same location as the
	  distribution would typically use. We expect this to be
	  /var/lib/libvirt/images/ for most distributions, however we can
	  customize this further if this is not true by adding further checks.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the qemu storage pool path. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM
	string "Libvirt storage pool path"
	default "/var/lib/libvirt/images/" if !LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default $(shell, scripts/cwd-append.sh vagrant) if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default "/opt/libvirt/images/" if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	help
	  The path to use for the libvirt storage pool path. Since kdevops uses
	  vagrant for virtualization this is also the path used to place the
	  additional nvme drives created. kdevops adds a postfix "kdevops" to
	  this directory as it wants to allow vagrant full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will let vagrant store images in /var/lib/libvirt/images/ and
	  the nvme qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/.vagrant/nvme_disks/guest-hostname/.

choice
	prompt "Libvirt URI"
	default LIBVIRT_URI_SYSTEM if !DISTRO_FEDORA
	default LIBVIRT_URI_SESSION if DISTRO_FEDORA

config LIBVIRT_URI_SYSTEM
	bool "Use qemu:///system for the URI"
	help
	  The first design behind libvirt is to use the system URI, that is,
	  qemu:///system. All 'system' URIs (be it qemu, lxc, uml, ...)
	  connect to the libvirtd daemon running as root which is launched at
	  system startup. Virtual machines created and run using 'system'
	  are usually launched as root, unless configured otherwise (for
	  example in /etc/libvirt/qemu.conf). A distribution can however still
	  allow users to use the system URI if they are added to the respective
	  groups to use libvirt, and this is the approach taken by kdevops when
	  this option is enabled.

	  You will definitely want to use qemu:///system if your VMs are
	  acting as servers. VM autostart on host boot only works for 'system',
	  and the root libvirtd instance has necessary permissions to use
	  proper networkings via bridges or virtual networks. qemu:///system
	  is generally what tools like virt-manager default to.

	  When this option is enabled vagrant's libvirt default built-in
	  URI is used along with the default network management interface,
	  libvirt socket, and the network interface assumed for bridging.

	  For more details on this refer to the libvirt wiki which still
	  advises in favor of the system URI over the session URI:

	  https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F

config LIBVIRT_URI_SESSION
	bool "Use qemu:///session for the URI"
	help
	  A second design consideration has been implemented into libvirt to
	  enable users to use libvirt without the libvirt daemon needing to
	  run as root. All 'session' URIs launch a libvirtd instance as your
	  local user, and all VMs are run with local user permissions.

	  The benefit of qemu:///session is that permission issues vanish:
	  disk images can easily be stored in $HOME, serial PTYs are owned by
	  the user, etc.

	  qemu:///session has a serious drawback: since the libvirtd instance
	  does not have sufficient privileges, the only out of the box network
	  option is qemu's usermode networking, which has nonobvious
	  limitations, so its usage is discouraged. More info on qemu
	  networking options: http://people.gnome.org/~markmc/qemu-networking.html
	  With regards to kdevops, if you use the session URI we don't
	  instantiate secondary interfaces with private IP addresses. This is
	  not a requirement for the currently supported workflows but if
	  you are doing custom networking stuff this may be more relevant for
	  you. Fedora defaults to the session URI.

	  When this option is enabled we modify vagrant's libvirt default
	  built-in URI for the session URI, and we also modify the default
	  network management interface to be virbr0, the default socket
	  is assumed to be /run/libvirt/libvirt-sock-ro. New kconfig options
	  can be added later to customize those further if we really need
	  to.

	  Please note that sensible defaults are enabled for your Linux
	  distribution, so if your distribution does not have session URI
	  set by default it means it doesn't support it yet and you should
	  expect things to not work, and put the work to fix / enhance that
	  somehow. That work likely is not on kdevops... but perhaps this
	  could be worng. Testing has be done with session support on debian
	  testing, Ubuntu 21.10 and they both have issues. Don't enable session
	  support manually unless you know what you are doing.

config LIBVIRT_URI_CUSTOM
	bool "Custom qemu URI"
	help
	  Select this option if you want to manually specify which URI to use.
	  In other words you know what you are doing.

endchoice

config LIBVIRT_URI_PATH
	string "Libvirt qemu URI to use"
	default "qemu:///system" if LIBVIRT_URI_SYSTEM || LIBVIRT_URI_CUSTOM
	default "qemu:///session" if LIBVIRT_URI_SESSION
	help
	  By default vagrant uses a qemu:///system URI which assumes the libvirt
	  daemon runs as a user other than the user which is running the vagrant
	  commands. Libvirt has support for running the libvirt daemon as other
	  users using session support. This will be modified to a session URI
	  if you enable LIBVIRT_URI_SESSION. You can however set this to
	  something different to suit your exact needs here. This is the value
	  passed to the vagrant-libvirt plugin libvirt.uri. You should not have
	  to modify this value if you selected LIBVIRT_URI_SYSTEM or
	  LIBVIRT_URI_SESSION and are starting from a fresh 'make mrproper'
	  setting on kdevops, the appropriate value will be set for you.
	  You should only have to modify this manually if you set
	  LIBVIRT_URI_CUSTOM and you know what you are doing.

config LIBVIRT_SYSTEM_URI_PATH
	string "Libvirt system qemu URI to use"
	default "qemu:///system"
	help
	  This is the URI of QEMU system connection, used to obtain the IP
	  address for management. This is used for the vagrant-libvirt plugin
	  libvirt.system_uri setting. If for whatever reason this needs to
	  be modified you can do so here. Even if you are using session
	  support you should leave this with the default qemu:///system setting
	  as this is still used to ensure your guest's IP address will be
	  communicated back to vagrant so it determines the guest is up and
	  you can ssh to it. Setting this to qemu:///session still gets the
	  guest up but vagrant won't know the guest is up, even though the
	  host can ssh to the guest. You should only modify this value if
	  you know what you are doing.

endif

if VAGRANT_VIRTUALBOX

choice
	prompt "Virtualbox storage pool path"
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL if !DISTRO_SUSE
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the vagrant storage pool path where
	  additional nvme drives will be created. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the directory where we'll have kdevops create additional nvme drives
	  for virtualbox to use. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM
	string "Virtualbox storage pool path"
	default $(shell, scripts/cwd-append.sh vagrant) if VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_CWD
	default "/opt/virtualbox/storage/" if VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM_MANUAL
	help
	  The path to use for creating additional nvme drives used by
	  virtualbox. kdevops adds a postfix "kdevops" to this directory as it
	  wants to allow vagrant full control over that directory. For instance
	  if this is /opt/virtualbox/storage/ kdevops will have virtualbox
	  create the nvme files under the directory
	  /opt/virtualbox/storage/kdevops/.vagrant/nvme_disks/guest-hostname/.

endif

config LIBVIRT_QEMU_GROUP
	string
	default "qemu" if !DISTRO_DEBIAN && !DISTRO_UBUNTU
	default "libvirt-qemu" if DISTRO_DEBIAN || DISTRO_UBUNTU

config KDEVOPS_STORAGE_POOL_PATH
	string
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM if VAGRANT_LIBVIRT
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM if VAGRANT_VIRTUALBOX

config QEMU_BIN_PATH
	string
	default QEMU_BIN_PATH_LIBVIRT if VAGRANT_LIBVIRT
	default "/usr/bin/qemu-system-x86_64" if !VAGRANT_LIBVIRT

config LIBVIRT_URI
	string
	default "qemu:///system" if !VAGRANT_LIBVIRT
	default LIBVIRT_URI_PATH if VAGRANT_LIBVIRT

config LIBVIRT_SYSTEM_URI
	string
	default "qemu:///system" if !VAGRANT_LIBVIRT
	default LIBVIRT_SYSTEM_URI_PATH if VAGRANT_LIBVIRT

config LIBVIRT_SESSION
	bool
	default LIBVIRT_URI_SESSION

# These defaults are only used when LIBVIRT_SESSION is true,
# but we need them to build our nodes file even if LIBVIRT_SESSION is
# disabled. We can provide new kconfig entries for these should we
# need them later. For now we default to what Fedora uses as that's
# the only distro using the session URI by default by now.
config LIBVIRT_SESSION_SOCKET
	string
	default "/run/libvirt/libvirt-sock-ro"

config LIBVIRT_SESSION_MANAGEMENT_NETWORK_DEVICE
	string
	default "virbr0"

config LIBVIRT_SESSION_PUBLIC_NETWORK_DEV
	string
	default "virbr0"

config HAVE_SUSE_VAGRANT
	bool
	default $(shell, scripts/check_distro_kconfig.sh suse)

choice
	prompt "Vagrant Guest Linux distribution to use"
	default VAGRANT_DEBIAN if !HAVE_SUSE_VAGRANT
	default VAGRANT_SUSE if HAVE_SUSE_VAGRANT

config VAGRANT_DEBIAN
	bool "Debian"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG if FSTESTS_XFS
	select HAVE_DISTRO_PREFERS_FSTESTS_WATCHDOG_KILL if FSTESTS_XFS
	help
	  This option will set the target guest to Debian.

config VAGRANT_OPENSUSE
	bool "OpenSUSE"
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BLKTESTS_PREFERS_MANUAL if KDEVOPS_WORKFLOW_ENABLE_BLKTESTS
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	help
	  This option will set the target guest to OpenSUSE.

config VAGRANT_SUSE
	bool "SUSE"
	depends on HAVE_SUSE_VAGRANT
	select HAVE_KDEVOPS_CUSTOM_DEFAULTS
	select HAVE_CUSTOM_DISTRO_HOST_PREFIX
	select HAVE_DISTRO_PREFERS_CUSTOM_HOST_PREFIX
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	select HAVE_DISTRO_BTRFS_PREFERS_MANUAL if FSTESTS_BTRFS
	select HAVE_DISTRO_BLKTESTS_PREFERS_MANUAL if KDEVOPS_WORKFLOW_ENABLE_BLKTESTS
	select HAVE_DISTRO_SUSE
	select HAVE_DISTRO_PREFERS_REGISTRATION
	select HAVE_DISTRO_REG_METHOD_TWOLINE
	select VAGRANT_INSTALL_PRIVATE_BOXES
	select HAVE_CUSTOM_KDEVOPS_GIT
	select HAVE_CUSTOM_KDEVOPS_GIT_DATA
	select HAVE_CUSTOM_KDEVOPS_DIR
	help
	  This option will set the target guest to SUSE. There is currently
	  no scriptable way to download vagrant images, however the images
	  are available for download via:

	    https://suse.com/download

config VAGRANT_FEDORA
	bool "Fedora"
	select HAVE_DISTRO_XFS_PREFERS_MANUAL if FSTESTS_XFS
	help
	  This option will set the target guest to Fedora.

endchoice

config HAVE_VAGRANT_BOX_VERSION
	bool
	default n

source "vagrant/Kconfig.debian"
source "vagrant/Kconfig.opensuse"
source "vagrant/Kconfig.fedora"

if HAVE_SUSE_VAGRANT
source "vagrant/Kconfig.suse"
endif # HAVE_SUSE_VAGRANT

config VAGRANT_BOX
	string "Vagrant box"
	default "debian/testing64" if VAGRANT_DEBIAN_TESTING64 || !VAGRANT
	default "debian/buster64" if VAGRANT_DEBIAN_BUSTER64
	default "opensuse/Tumbleweed.x86_64" if VAGRANT_OPENSUSE_X86_64_TW
	default "fedora/35-cloud-base" if VAGRANT_FEDORA_X86_64_35_CLOUD_BASE
	default "opensuse/Leap-15.3.x86_64" if VAGRANT_OPENSUSE_X86_64_LEAP_15_3
	default "Leap-15.4.x86_64" if VAGRANT_OPENSUSE_X86_64_LEAP_15_4
	default VAGRANT_SUSE_BOX if VAGRANT_SUSE
	help
	  The vagrant box to use.

config VAGRANT_PREFERRED_KERNEL_CI_SUBJECT_TOPIC
	string
	default VAGRANT_BOX if VAGRANT_DEBIAN_BUSTER64


config HAVE_VAGRANT_BOX_URL
	bool

if HAVE_VAGRANT_BOX_URL

config VAGRANT_BOX_URL
	string
	depends on HAVE_VAGRANT_BOX_URL
	default VAGRANT_SUSE_BOX_URL if HAVE_SUSE_VAGRANT
	default "https://download.opensuse.org/repositories/Virtualization:/Appliances:/Images:/openSUSE-Leap-15.4/images/boxes/Leap-15.4.x86_64.json" if VAGRANT_OPENSUSE_X86_64_LEAP_15_4

endif # HAVE_VAGRANT_BOX_URL

if HAVE_VAGRANT_BOX_VERSION

config VAGRANT_BOX_VERSION
	string "Vagrant box version"
	default "1.0.20210915" if VAGRANT_OPENSUSE_X86_64_TW_1020210915
	default "1.0.20210203" if VAGRANT_OPENSUSE_X86_64_TW_1020210203
	default "1.0.20200714" if VAGRANT_OPENSUSE_X86_64_TW_1020200714
	default "1.0.20210203" if VAGRANT_OPENSUSE_X86_64_TW_1020210203
	default "32.20200312.0" if VAGRANT_FEDORA_32_202003120
	help
	  The vagrant box version to use. This is set for you depending on the
	  image you select. You can manually override the version we have last
	  tested here.

endif # HAVE_VAGRANT_BOX_VERSION

if !HAVE_VAGRANT_BOX_VERSION

config VAGRANT_BOX_VERSION
	string
	default ""

endif # !HAVE_VAGRANT_BOX_VERSION

config VAGRANT_LIBVIRT_INSTALL
	bool "Installs libvirt"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the ansible role which installs
	  libvirt for you will be run. The goal will be to ensure you have
	  libvirt installed and running.

config VAGRANT_LIBVIRT_CONFIGURE
	bool "Configure libvirt so you spawn guests as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the ansible role which configures
	  libvirt for you will be run. This typically just requires adding the
	  user to a specific set of groups. The user must log out and back
	  in again, to ensure the new group takes effect. The goal in the
	  configuration will be to ensure you can use libvirt to spawn guests
	  as a regular user. You are encouraged to say y here unless you know
	  what you are doing or you already know this works. If you are unsure,
	  the litmus test for this is if you can run vagrant up, on any public
	  demo box available.

config VAGRANT_LIBVIRT_VERIFY
	bool "Verify that a user can spawn libvirt as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  To enable a user to be able to spawn libvirt guests as a regular user
	  a user is typically added to a few groups. These groups are not
	  effective immediately, and so before a user can assume that they
	  use vagrant they must verify that the required groups are effective.
	  If you enable this option, we will spawn an ansible role that will
	  verfify and ensure that your user is already part of these groups.
	  You can safely say yes here.

config VAGRANT_INSTALL_PRIVATE_BOXES
	bool "Install private vagrant boxes"
	default y
	help
	  If this option is enabled then the ansible role which installs
	  additional vagrant boxes will be run. This is useful if for example,
	  you have private vagrant boxes available and you want to use them.
	  You can safely disable this option if you are using only public
	  vagrant boxes. Enabling this option is safe as well, given no
	  private boxes would be defined, and so nothing is done.

config QEMU_ENABLE_NVME_ZNS
	bool "Enable Qemu NVMe ZNS drives"
	depends on VAGRANT_LIBVIRT
	default n
	help
	  If this option is enabled then you can enable NVMe ZNS drives on the
	  guests.

config QEMU_CUSTOM_NVME_ZNS
	bool "Customize Qemu NVMe ZNS settings"
	depends on QEMU_ENABLE_NVME_ZNS
	default n
	help
	  If this option is enabled then you will be able to modify the defaults
	  used for the 2 NVMe ZNS drives we create for you. By default we create
	  two NVMe ZNS drives with 100 GiB of total size, each zone being
	  128 MiB, and so you end up with 800 total zones. The zone capacity
	  equals the zone size. The default zone size append limit is also
	  set to 0, which means the zone append size limit will equal to the
	  maximum data transfer size (MDTS). The default logical and physical
	  block size of 4096 bytes is also used. If you want to customize any
	  of these ZNS settings for the drives we bring up enable this option.

	  If unsure say N.

if QEMU_CUSTOM_NVME_ZNS

config QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE
	int "Qemu ZNS storage nvme drive size"
	default 102400
	help
	  The size of the qemu nvme ZNS drive to expose. We expose 2 NVMe
	  ZNS drives of 100 GiB by default. This value chagnes its size.
	  100 GiB is a sensible default given most full fstests require about
	  50 GiB of data writes.

config QEMU_CUSTOM_NVME_ZONE_ZASL
	int "Qemu ZNS zasl - zone append size limit power of 2"
	default 0
	help
	  This is the zone append size limit. If left at 0 qemu will use
	  the maximum data transfer size (MDTS) for the zone size append limit.
	  Otherwise if this value is set to something other than 0, then the
	  zone size append limit will equal to 2 to the power of the value set
	  here multiplied by the minimum memory page size (4096 bytes) but the
	  qemu promises this value cannot exceed the maximum data transfer size.

config QEMU_CUSTOM_NVME_ZONE_SIZE
	string "Qemu ZNS storage nvme zone size"
	default "128M"
	help
	  The size the the qemu nvme ZNS zone size. The number of zones are
	  implied by the driver size / zone size. If there is a remainder
	  technically that should go into another zone with a smaller zone
	  capacity.

config QEMU_CUSTOM_NVME_ZONE_CAPACITY
	int "Qemu ZNS storage nvme zone capacity"
	default 0
	help
	  The size to use for the zone capacity. This may be smaller or equal
	  to the zone size. If set to 0 then this will ensure the zone
	  capacity is equal to the zone size.

config QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE
	int "Qemu ZNS storage nvme zone max active"
	default 0
	help
	  The max numbe of active zones. The default of 0 means all zones
	  can be active at all times.

config QEMU_CUSTOM_NVME_ZONE_MAX_OPEN
	int "Qemu ZNS storage nvme zone max open"
	default 0
	help
	  The max numbe of open zones. The default of 0 means all zones
	  can be opened at all times. If the number of active zones is
	  specified this value must be less than or equal to that value.

config QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCKSIZE
	int "Qemu ZNS storage nvme physical block size"
	default 4096
	help
	  The physical block size to use for ZNS drives. This ends up
	  what is put into the /sys/block/<disk>/queue/physical_block_size
	  and is the smallest unit a physical storage device can write
	  atomically. It is usually the same as the logical block size but may
	  be bigger. One example is SATA drives with 4KB sectors that expose a
	  512-byte logical block size to the operating system. For stacked
	  block devices the physical_block_size variable contains the maximum
	  physical_block_size of the component devices.

config QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCKSIZE
	int "Qemu ZNS storage nvme logical block size"
	default 4096
	help
	  The logical block size to use for ZNS drives. This ends up what is
	  put into the /sys/block/<disk>/queue/logical_block_size and the
	  smallest unit the storage device can address. It is typically 512
	  bytes.

endif # QEMU_CUSTOM_NVME_ZNS

config VAGRANT_ENABLE_ZNS
	bool
	default y if QEMU_ENABLE_NVME_ZNS

config QEMU_NVME_ZONE_DRIVE_SIZE
	int
	default 102400 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_ZASL
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_ZASL if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_SIZE
	string
	default "128M" if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_CAPACITY
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_CAPACITY if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_ACTIVE
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_OPEN
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_OPEN if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_PHYSICAL_BLOCKSIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCKSIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_LOGICAL_BLOCKSIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCKSIZE if QEMU_CUSTOM_NVME_ZNS

endif # VAGRANT
